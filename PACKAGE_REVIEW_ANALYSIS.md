# evolveBO Package Review: Literature Fidelity, Assumptions, and Performance Analysis

**Review Date**: 2025-11-04
**Reviewer**: Claude Code (Automated Analysis)
**Package Version**: 0.2.0

## Executive Summary

This review analyzes the evolveBO package implementation against Bayesian optimization literature, identifies implicit assumptions, evaluates performance opportunities, and provides recommendations on the multi-fidelity strategy. Overall, the package implements sound methodology with a few areas for improvement.

**Key Findings**:
- ‚úÖ Surrogate modeling approach is well-grounded in literature
- ‚úÖ Constraint handling via ECI is reasonable, though simpler than state-of-art
- ‚ö†Ô∏è Multi-fidelity strategy uses **staged** approach where **adaptive** would be more aligned with literature
- ‚ö†Ô∏è Several implicit assumptions may limit applicability
- üéØ Multiple performance optimization opportunities identified

---

## 1. Literature Fidelity Analysis

### 1.1 Surrogate Modeling (‚úÖ STRONG)

**Implementation**: `R/surrogates.R` lines 1-232
- Uses DiceKriging with Mat√©rn 5/2 kernel
- Heteroskedastic GP when variance estimates provided
- Falls back to homoskedastic GP with nugget when variance unavailable

**Literature Alignment**:
- **Heteroskedastic GPs**: Well-established approach (Ankenman et al. 2010 "Stochastic Kriging"; Binois et al. 2018 "hetGP")
- **Mat√©rn 5/2 kernel**: Standard choice, twice differentiable, good balance of smoothness
- **Variance handling**: Correct use of Monte Carlo variance as noise parameter

**Assessment**: Implementation is faithful to best practices. The variance estimation using Welford's algorithm is particularly well-designed and memory-efficient.

**Minor concerns**:
1. No automatic kernel selection - fixed to user choice (default Mat√©rn 5/2)
2. Aggregation via simple averaging (line 68-86) - could use weighted averaging by inverse variance
3. No outlier detection before fitting

### 1.2 Acquisition Function (‚ö†Ô∏è MODERATE)

**Implementation**: `R/acquisition.R` lines 1-82
- Expected Constrained Improvement (ECI)
- ECI = EI(x) √ó P(feasible | x)

**Literature Alignment**:
- ECI is a reasonable approach for constrained optimization
- Simpler than more sophisticated methods:
  - **Expected Hypervolume Improvement (EHVI)**: Better for multi-objective problems
  - **Constrained Expected Improvement (CEI)**: More principled treatment of constraints
  - **qEHVI**: Batch version with better theoretical properties

**Code Review** (`acq_eci` function, lines 9-38):
```r
prob_feas <- prob_feasibility(mu, sigma, constraint_tbl)
if (any(mu_obj < best_feasible)) {
  improvement <- pmax(best_feasible - mu_obj, 0)
  Z <- improvement / sigma_obj
  ei <- improvement * pnorm(Z) + sigma_obj * dnorm(Z)
} else {
  ei <- sigma_obj  # Pure exploration if no improvement possible
}
acq_eci <- ei * prob_feas
```

**Issues Identified**:
1. **Line 19-20**: When no feasible solution found yet (`best_feasible = Inf`), falls back to `sigma_obj` (exploration only)
   - **Problem**: Ignores constraint information entirely
   - **Better approach**: Use constraint violation magnitude (e.g., sum of constraint violations weighted by probability)

2. **Line 35**: Uses product of univariate probabilities for feasibility
   - **Assumption**: Constraints are independent (may not hold)
   - **Alternative**: Could sample from joint distribution (slower but more accurate)

3. **Missing**: No diversity mechanism for batch selection (`q > 1`)
   - Currently selects top-q by acquisition value
   - Can lead to redundant evaluations in same region
   - Literature: Should use hallucinated observations or local penalization

**Assessment**: Functional but could be more sophisticated. The lack of batch diversity mechanism is the most significant gap.

### 1.3 Multi-Fidelity Strategy (‚ö†Ô∏è NEEDS REVISION)

**Implementation**: `R/bo_calibrate.R` lines 406-467

**Current Approach**: **Staged/Fixed-Schedule** (via `select_fidelity_staged`)
```r
select_fidelity_staged <- function(prob_feasible, cv_estimate, iter, fidelity_levels) {
  # Stage 1: Iterations 1-30 ‚Üí always low fidelity
  if (iter <= 30) return("low")

  # Stage 3: Iterations 101+ ‚Üí high if prob_feasible > 0.6
  if (iter > 100 && prob_feasible > 0.6 && "high" %in% names(fidelity_levels)) {
    return("high")
  }

  # Stage 2: Iterations 31-100 ‚Üí adaptive based on CV and feasibility
  if (cv_estimate > 0.18 && prob_feasible >= 0.2 && prob_feasible <= 0.8) {
    return("high")
  }

  # Default logic...
}
```

**Literature on Multi-Fidelity BO**:

1. **Multi-Fidelity Knowledge Gradient (MFKG)** (Frazier et al., Wu & Frazier 2016)
   - Key insight: Choose fidelity by maximizing information gain per unit cost
   - Decision: argmax_{fidelity} E[VoI(x, fidelity)] / cost(fidelity)
   - Uses recursive formulation to estimate value of information

2. **Continuous Multi-Fidelity with hetGP** (Binois et al. 2018)
   - Treats fidelity as continuous input dimension
   - GP learns correlation between fidelity levels
   - No need for explicit fidelity selection rules

3. **Cost-Aware Acquisition Functions** (Swersky et al. 2013, Kandasamy et al. 2016)
   - Modify acquisition to account for cost: Œ±(x, fidelity) / cost(fidelity)^œâ
   - œâ ‚àà [0,1] trades off cost vs information

**Problems with Current Staged Approach**:

1. **Hard iteration thresholds are arbitrary**
   - Why 30 iterations for exploration? Why not 20 or 40?
   - Optimal duration depends on problem dimension, constraint difficulty, noise level
   - Fixed schedule cannot adapt to problem characteristics

2. **Ignores cost-benefit tradeoff**
   - High fidelity costs 50√ó more than low (10,000 vs 200 reps)
   - Should only use high fidelity when information gain justifies cost
   - Current: Uses high fidelity based on iteration number, not value of information

3. **CV threshold (0.18) is magic number**
   - No theoretical justification
   - Should be relative to current best value and constraint margins

4. **Doesn't leverage information across fidelities**
   - Treats each fidelity independently
   - Could use GP to model correlation between fidelities

**Legacy Function** (`select_fidelity`, lines 456-467):
- Even simpler: pure threshold on feasibility probability
- No cost consideration at all
- Not recommended for use

**Assessment**: The staged approach is a **heuristic** that may work in practice but lacks theoretical foundation. Literature strongly favors **adaptive/cost-aware** selection.

### 1.4 Constraint Handling (‚úÖ ADEQUATE)

**Implementation**: `R/constraints.R`
- Probabilistic feasibility: Product of univariate Gaussian CDFs
- Supports "ge" and "le" constraints

**Literature Alignment**:
- Standard approach for GP-based constrained BO
- Assumption: Constraints are independent under GP posterior
- Alternative: Joint sampling (more accurate but slower)

**Assessment**: Adequate for most applications. Independence assumption is common in literature.

---

## 2. Implicit Assumptions Analysis

### 2.1 Stationarity Assumption (CRITICAL)

**Where**: GP kernel assumes stationary covariance
**Impact**: May fail if operating characteristics have:
- Discontinuities (e.g., trial stops early in some regions)
- Different smoothness in different regions
- Local patterns not captured by global kernel

**Mitigation**:
- Consider non-stationary kernels (e.g., deep GPs, neural processes)
- Or: Use local GP models (treed GP, partition models)

### 2.2 Low-Dimensional Assumption (MODERATE)

**Where**: Latin Hypercube Sampling for candidate pool (2000 candidates)
**Impact**: LHS becomes less space-filling in high dimensions (curse of dimensionality)
- d=2-5: Excellent coverage
- d=6-10: Good coverage
- d>10: Poor coverage, may miss good regions

**Mitigation**:
- Use Sobol sequences (better uniformity)
- Increase candidate pool size with dimension
- Consider dimension reduction (active subspace)

### 2.3 Constraint Independence (MODERATE)

**Where**: `prob_feasibility()` computes product of marginal probabilities
**Formula**: P(all constraints satisfied) = ‚àè P(constraint_i satisfied)

**Assumption**: Constraints are independent under GP posterior
**Reality**: Often correlated (e.g., power and type-I error both depend on threshold)

**Impact**:
- Overestimates feasibility if constraints negatively correlated
- Underestimates feasibility if positively correlated

**Mitigation**:
- Sample from joint GP posterior (Monte Carlo)
- Only 100-200 samples needed for stable estimates

### 2.4 Smooth Objective Assumption (HIGH)

**Where**: GP with Mat√©rn 5/2 kernel (twice differentiable)
**Impact**: If objective has discontinuities or sharp ridges:
- GP will poorly approximate function
- May oscillate or over-smooth

**Mitigation**:
- Option for Mat√©rn 3/2 (once differentiable) or Mat√©rn 1/2 (continuous only)
- Check sensitivity analysis for large gradient changes

### 2.5 Constant Noise Within Fidelity (LOW)

**Where**: Heteroskedastic GP assumes variance depends on fidelity but not on location in parameter space
**Reality**: Some regions may have higher intrinsic variability (e.g., near decision boundaries)

**Impact**: May over-explore low-noise regions, under-explore high-noise regions
**Mitigation**: Could use fully heteroskedastic GP (hetGP package), but computationally expensive

### 2.6 No Parameter Interactions Assumed (LOW)

**Where**: LHS initial design treats parameters independently
**Impact**: May miss interaction effects initially
**Mitigation**: Initial design size should be ‚â• 4d to capture interactions

---

## 3. Performance Optimization Opportunities

### 3.1 Batch Selection with Diversity (HIGH PRIORITY)

**Current**: Lines 130-140 in `bo_calibrate.R`
```r
# Select top q candidates by acquisition value
chosen_indices <- order(acq_scores, decreasing = TRUE)[1:q]
chosen_units <- candidates[chosen_indices, , drop = FALSE]
```

**Problem**: Batch points may cluster in same region ‚Üí redundant information

**Solution**: Use one of:
1. **Local penalization** (Gonz√°lez et al. 2016)
   - After selecting x1, penalize acquisition near x1
   - Repeat for x2, x3, etc.

2. **Constant Liar (CL) / Kriging Believer (KB)** (Ginsbourger et al. 2010)
   - Hallucinate observation at x1 (set to current best or posterior mean)
   - Refit GP, select x2
   - Repeat

3. **qEI / qECI** (Ginsbourger et al. 2010, Wilson et al. 2018)
   - Analytically compute batch acquisition
   - Expensive but theoretically optimal

**Expected Impact**: 10-20% fewer evaluations to convergence

### 3.2 Adaptive Candidate Pool Size (MEDIUM PRIORITY)

**Current**: Fixed 2000 candidates regardless of dimension or iteration

**Opportunity**:
- Early iterations: Can use fewer candidates (exploration dominated)
- Late iterations: May need more candidates (optimization refinement)
- High dimensions: Need more candidates for coverage

**Suggestion**:
```r
n_candidates <- max(2000, 500 * length(bounds))  # Scale with dimension
```

### 3.3 Warm-Starting GP Hyperparameters (MEDIUM PRIORITY)

**Current**: Each `fit_surrogates()` call optimizes hyperparameters from scratch

**Opportunity**:
- Use previous iteration's hyperparameters as starting point
- Hyperparameters evolve slowly, warm-start saves ~30-50% fitting time

**Implementation**:
```r
if (!is.null(prev_surrogates)) {
  # Extract previous hyperparameters
  theta_init <- coef(prev_surrogates[[metric]])$theta
} else {
  theta_init <- NULL  # First iteration: default initialization
}
```

### 3.4 Parallel Surrogate Fitting (LOW PRIORITY, EASY WIN)

**Current**: Fits GPs for each metric sequentially

**Opportunity**: Metrics are independent ‚Üí can fit in parallel
```r
library(furrr)
plan(multisession, workers = 4)

surrogates <- future_map(metrics, ~fit_single_surrogate(...))
```

**Expected Impact**: 2-4√ó speedup in surrogate fitting (usually 5-10% of total time)

### 3.5 Adaptive Initial Design Size (MEDIUM PRIORITY)

**Current**: User specifies `n_init`

**Suggestion**: Provide guidance or adaptive default
```r
n_init_suggested <- max(4 * d, 10)  # 4√ó dimension, minimum 10
```

**Justification**: Literature recommends 2-5√ó dimension for stable GP

### 3.6 Early Stopping (MEDIUM PRIORITY)

**Current**: Always runs until budget exhausted

**Opportunity**: Stop if:
1. Best value hasn't improved in k iterations
2. GP uncertainty below threshold in promising region
3. All candidate points have low acquisition value

**Implementation**:
```r
if (iter > 20 && max(acq_scores) < tolerance) {
  message("Converged: acquisition values below threshold")
  break
}
```

### 3.7 Constraint Relaxation (LOW PRIORITY, ADVANCED)

**Current**: Hard constraints

**Opportunity**: When no feasible points found, temporarily relax constraints:
- Minimize constraint violation
- Once feasible region found, switch to optimizing objective

**Literature**: "Two-phase" approach (Parr et al. 2012)

---

## 4. Multi-Fidelity Strategy Recommendation

### Current State

The package implements **two** fidelity selection strategies:

1. **`select_fidelity_staged()`** (lines 418-452) - Currently used
   - Stage 1 (iter 1-30): Low fidelity
   - Stage 2 (iter 31-100): Adaptive based on CV and feasibility
   - Stage 3 (iter 101+): High fidelity in promising regions

2. **`select_fidelity()`** (lines 456-467) - Legacy, threshold-based
   - P(feasible) ‚â• 0.75 ‚Üí high
   - P(feasible) ‚â• 0.4 ‚Üí med
   - Otherwise ‚Üí low

### Literature Recommendation: **Adaptive Cost-Aware Selection**

The literature consensus favors **adaptive** fidelity selection that balances:
- Information gain
- Computational cost
- Current optimization state

**Recommended Approach**: Modified Multi-Fidelity Knowledge Gradient (MFKG)

### Proposed Implementation

```r
select_fidelity_adaptive <- function(prob_feasible, cv_estimate,
                                     acq_value, best_obj,
                                     fidelity_levels, iter) {
  if (length(fidelity_levels) == 1L) {
    return(names(fidelity_levels))
  }

  # Compute value-per-cost for each fidelity
  fidelity_names <- names(fidelity_levels)
  costs <- as.numeric(fidelity_levels)

  # Heuristic value function
  # High value when: (1) high acquisition, (2) high uncertainty, (3) near feasibility boundary
  uncertainty_factor <- cv_estimate  # Higher CV ‚Üí more uncertainty
  boundary_factor <- 1 - abs(2 * prob_feasible - 1)  # Max at 0.5, min at 0 and 1

  # Diminishing returns on acquisition value
  value_score <- log1p(acq_value) * uncertainty_factor * (1 + boundary_factor)

  # Normalize costs to [0,1] scale
  cost_normalized <- costs / max(costs)

  # Value per cost (higher is better)
  # Use power exponent to control cost sensitivity
  cost_exponent <- if (iter < 20) 0.5 else 0.8  # More cost-sensitive later
  value_per_cost <- value_score / (cost_normalized ^ cost_exponent)

  # Select fidelity with highest value-per-cost
  # But: force low fidelity with some probability for exploration
  exploration_prob <- max(0.1, 0.5 - iter / 100)  # Decay from 50% to 10%

  if (runif(1) < exploration_prob) {
    return(fidelity_names[1])  # Low fidelity
  } else {
    best_idx <- which.max(value_per_cost)
    return(fidelity_names[best_idx])
  }
}
```

**Key Features**:
1. **Cost-aware**: Divides value by cost
2. **Adaptive**: Changes behavior based on optimization state
3. **Exploration**: Maintains randomness to avoid premature convergence
4. **No hard thresholds**: Smooth transitions between fidelities

### Alternative: Continuous Fidelity with hetGP

**More sophisticated but requires refactoring**:

1. Treat fidelity as continuous parameter (n_rep ‚àà [200, 10000])
2. Use hetGP to model correlation across fidelities
3. Optimize (x, fidelity) jointly

**Pros**:
- Theoretically optimal
- Learns fidelity correlation from data
- No hand-tuned parameters

**Cons**:
- Requires significant refactoring
- Computationally more expensive
- hetGP fitting can be unstable

### Specific Recommendations

**SHORT TERM** (Immediate):
1. ‚úÖ **Keep staged approach as default** but:
   - Make iteration thresholds configurable: `stage_thresholds = c(explore = 30, refine = 100)`
   - Add documentation explaining these are heuristics
   - Expose `select_fidelity_method = c("staged", "adaptive", "threshold")` argument

2. ‚úÖ **Implement adaptive method** (above) as alternative option

3. ‚úÖ **Add cost parameter**: Allow users to specify relative costs
   ```r
   fidelity_costs = c(low = 1, med = 5, high = 50)
   ```

**MEDIUM TERM** (Next version):
1. Conduct ablation study comparing:
   - Staged (current)
   - Adaptive (proposed)
   - Threshold (legacy)
   - Fixed high-fidelity (baseline)

2. Default to best-performing method based on empirical results

3. Consider implementing continuous fidelity via hetGP

**LONG TERM** (Future research):
1. Implement true MFKG with recursive value computation
2. Explore multi-fidelity qECI for batch optimization
3. Add automatic fidelity detection from simulator timing

---

## 5. Code-Specific Issues

### 5.1 Potential Numerical Instability

**Location**: `R/acquisition.R` line 35
```r
Z <- improvement / sigma_obj
```

**Issue**: Division by zero if `sigma_obj = 0` (can happen at observed points)
**Fix**: Add small epsilon
```r
Z <- improvement / (sigma_obj + 1e-10)
```

### 5.2 Missing Input Validation

**Location**: `R/bo_calibrate.R` lines 48-85
- Should validate `q <= budget - n_init`
- Should check `objective %in% names(sim_fun(theta_test))`
- Should validate constraint format

### 5.3 Integer Parameter Rounding Timing

**Location**: `R/bo_calibrate.R` line 120
```r
if (!is.null(integer_params)) {
  theta <- round_integer_params(theta, integer_params)
}
```

**Issue**: Rounding happens AFTER acquisition optimization
**Problem**: Acquisition was optimized for continuous values, rounding changes point
**Better approach**: Optimize on discrete grid for integer parameters, or use discrete BO kernel

### 5.4 Aggregation Could Be Weighted

**Location**: `R/surrogates.R` lines 68-86

Currently averages repeated evaluations equally:
```r
metrics_value <- purrr::map_dbl(metrics, mean, na.rm = TRUE)
```

**Improvement**: Weight by inverse variance (more reliable estimates get higher weight)
```r
if (!all(is.na(variances))) {
  weights <- 1 / (variances + 1e-8)
  metrics_value <- sum(metrics * weights) / sum(weights)
}
```

---

## 6. Positive Aspects (What's Done Well)

1. ‚úÖ **Excellent variance estimation framework** - Welford's algorithm is state-of-art
2. ‚úÖ **Comprehensive testing** - Good coverage of core functionality
3. ‚úÖ **Modular design** - Easy to extend (e.g., new acquisition functions)
4. ‚úÖ **Documentation** - Extensive inline docs and vignettes
5. ‚úÖ **Diagnostic tools** - Sensitivity analysis, benchmarking, ablation studies
6. ‚úÖ **Memory efficient** - Welford's algorithm avoids storing all samples
7. ‚úÖ **Progress tracking** - Good user feedback during optimization
8. ‚úÖ **Seed management** - Proper RNG handling for reproducibility

---

## 7. Summary of Recommendations

### Priority: CRITICAL

1. **Add batch diversity mechanism** (Section 3.1)
   - Implement local penalization or constant liar
   - Expected: 10-20% efficiency improvement

2. **Implement adaptive fidelity selection** (Section 4)
   - Add cost-aware acquisition-based selection
   - Make staged vs adaptive user-configurable

3. **Fix numerical stability issues** (Section 5.1)
   - Add epsilon to prevent division by zero

### Priority: HIGH

4. **Improve infeasible region handling** (Section 1.2)
   - When no feasible points, optimize constraint satisfaction
   - Don't ignore constraint information

5. **Add input validation** (Section 5.2)
   - Validate budget, objective, constraints before starting

6. **Add suggested defaults for n_init** (Section 3.5)
   - Provide dimension-based recommendation

### Priority: MEDIUM

7. **Warm-start GP hyperparameters** (Section 3.3)
   - 30-50% speedup in surrogate fitting

8. **Adaptive candidate pool size** (Section 3.2)
   - Scale with dimension

9. **Early stopping criterion** (Section 3.6)
   - Stop when converged, save budget

10. **Fix integer parameter handling** (Section 5.3)
    - Round before acquisition, or use discrete kernel

### Priority: LOW

11. **Parallel surrogate fitting** (Section 3.4)
12. **Weighted aggregation** (Section 5.4)
13. **Constraint relaxation** (Section 3.7)
14. **Non-stationary kernels** (Section 2.1)

---

## 8. Conclusion

The evolveBO package is **well-implemented** with strong foundations in Bayesian optimization literature. The heteroskedastic GP approach with Welford's algorithm is particularly sophisticated. However, there are opportunities to improve:

1. **Multi-fidelity strategy**: Move from staged to adaptive cost-aware selection
2. **Batch optimization**: Add diversity mechanism
3. **Robustness**: Better handling of infeasible regions and edge cases

**Overall Assessment**: 7.5/10
- **Strengths**: Variance handling, modularity, documentation
- **Weaknesses**: Batch selection, fidelity strategy, numerical edge cases

**Recommendation**: Address CRITICAL and HIGH priority items in next version. The package is production-ready but would benefit from these enhancements for optimal performance.

---

## References

1. Ankenman et al. (2010). "Stochastic Kriging for Simulation Metamodeling"
2. Binois et al. (2018). "hetGP: Heteroskedastic Gaussian Process Modeling and Design"
3. Frazier et al. (2016). "Multi-Information Source Optimization"
4. Ginsbourger et al. (2010). "Kriging is well-suited to parallelize optimization"
5. Gonz√°lez et al. (2016). "Batch Bayesian Optimization via Local Penalization"
6. Kandasamy et al. (2016). "Multi-fidelity Gaussian Process Bandit Optimisation"
7. Parr et al. (2012). "Infill sampling criteria for surrogate-based optimization"
8. Swersky et al. (2013). "Multi-Task Bayesian Optimization"
9. Wilson et al. (2018). "Maximizing acquisition functions for Bayesian optimization"
10. Wu & Frazier (2016). "The parallel knowledge gradient method for batch Bayesian optimization"
